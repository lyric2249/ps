
# 중복 제거용 앵커
# x-airflow-image: &airflow_image apache/airflow:3.1.0-python3.13
# x-airflow-image: &airflow_image apache/airflow:3.1.0-python3.12
x-airflow-image: &airflow_image apache/airflow:2.11.0-python3.12




x-airflow-vols: &airflow_vols
  - ./airflow/dags:/opt/airflow/dags:ro
  - ./airflow/plugins:/opt/airflow/plugins:ro
  - ./airflow/requirements.txt:/opt/airflow/requirements.txt:ro
  - ./airflow/airflow_logs:/opt/airflow/logs
  - ./airflow/webserver_config.py:/opt/airflow/webserver_config.py:ro
  - ./airflow/boot.sh:/opt/boot.sh:ro

# Airflow 공통 환경변수 (DB URI는 .env에서 AIRFLOW_DB_URI로 넘김 권장)
x-airflow-common: &airflow_common
  image: *airflow_image
  env_file: [.env]
  user: "50000:0"
  networks: [edge]
  restart: unless-stopped
  volumes: *airflow_vols


# Postgres 공통
x-pg-env: &pg_env
  image: postgres:17
  restart: unless-stopped
  networks: [edge]


# 서브도메인 정의
x-superset-baseurl: &superset_baseurl "https://airflow.${PUBLIC_HOST}/"
x-airflow-baseurl: &airflow_baseurl "https://airflow.${PUBLIC_HOST}/"


services:

  # =========================
  # Postgres
  # =========================
  postgres-meta:
    <<: *pg_env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    env_file: .env
    volumes:
      - meta_pg:/var/lib/postgresql/data
      - ./postgres-meta/initdb.sh:/docker-entrypoint-initdb.d/10-initdb.sh:ro
    ports: ["127.0.0.1:5432:5432"]   # 로컬 전용
    healthcheck:
        test: ["CMD-SHELL", "pg_isready -q -h 127.0.0.1 -p 5432 || exit 1"]
        interval: 5s
        timeout: 5s
        retries: 3


  postgres-data:
    <<: *pg_env
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_USER}
      POSTGRES_DB: superset
    volumes:
      - data_pg:/var/lib/postgresql/data
    ports: ["127.0.0.1:5434:5432"]   # 로컬 전용

    



  # =========================
  # mongo2
  # =========================
  mongo:
    image: mongo:7
    container_name: mongo
    restart: unless-stopped
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_DB_USERNAME}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_DB_PASSWORD}
    # expose: ["27017"]
    ports: ["127.0.0.1:27017:27017"]   # 로컬 전용
    volumes:
      - ./mongo/data/db:/data/db
    networks: [edge]


  # =========================
  # Redis
  # =========================
  redis:
    image: redis:7.2-bookworm
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    expose: ["6379"]
    restart: always
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 20
    networks: [edge]

  # =========================
  # flower
  # =========================

  # flower:
  #   <<: *airflow_common
  #   command: celery flower
  #   profiles:
  #     - flower
  #   ports:
  #     - "5555:5555"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   restart: always
  #   depends_on:
  #     airflow-init:
  #       condition: service_completed_successfully


  # =========================
  # Superset
  # =========================
  superset:
    # image: ghcr.io/lyric2249/superset-custom:latest
    build:
      context: superset              # Dockerfile.superset 있는 디렉터리
      dockerfile: Dockerfile.superset

    env_file: .env
    environment:
      PUBLIC_HOST: ${PUBLIC_HOST}
      PYTHONPATH: /app/pythonpath:/app
      SUPERSET_API_BASEURL: *superset_baseurl
      SQLALCHEMY_DATABASE_URI: ${SUPERSET_DB_URI}
      SUPERSET__SQLALCHEMY_DATABASE_URI: ${SUPERSET_DB_URI}
      EXTRA_PIP_PACKAGES: authlib

      # (선택) Redis 캐시/큐 사용 시 superset_config.py에서 연결
      # e.g. redis://redis:6379/1
    expose: ["8088"]
    volumes:
      - superset_home:/app/superset_home
      - ./superset/superset_config.py:/app/pythonpath/superset_config.py:ro
      - ./superset/custom_sso_security_manager.py:/app/pythonpath/custom_sso_security_manager.py:ro
      - ./superset/boot.sh:/docker-entrypoint-initdb.d/10_boot.sh:ro
    depends_on:
      postgres-meta:
        condition: service_healthy
      redis:
        condition: service_started
    user: "1000:1000"
    restart: unless-stopped
    networks: [edge]



  # =========================
  # Airflow 211 (기본: LocalExecutor)
  # =========================
  airflow-211-init:
    <<: *airflow_common
    # env_file: [.env]
    entrypoint: ["/bin/bash","-lc"]
    command: ["airflow db migrate;"]
    # command: >
    #   -lc "
    #   set -euo pipefail;
    #   echo '[init] running airflow db migrate';
    #   airflow db migrate;
    #   echo '[init] creating admin user (idempotent)';
    #   airflow users create
    #     --username ${AIRFLOW_ADMIN_USER:-admin}
    #     --password ${AIRFLOW_ADMIN_PASSWORD:-admin}
    #     --firstname Admin --lastname User
    #     --role Admin --email ${AIRFLOW_ADMIN_EMAIL:-admin@example.com} || true;
    #   echo '[init] done';
    #   "
    restart: "no"                  # ← init 컨테이너는 재시작 금지
    depends_on:
      postgres-meta:
        condition: service_healthy


  airflow-211-webserver:
    <<: *airflow_common
    container_name: airflow-webserver
    ports:
      - "8080:8080"
    command: >
      bash -lc "exec airflow webserver"
    # command: ["bash","-lc","exec /home/airflow/.local/bin/airflow webserver"]
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 60s
    restart: unless-stopped
    depends_on:
      postgres-meta:
        condition: service_healthy
      airflow-211-init:
        condition: service_completed_successfully


  airflow-211-scheduler:
    <<: *airflow_common
    container_name: airflow-scheduler
    command: >
      bash -lc "exec airflow scheduler"
    # command: ["bash","-lc","exec /home/airflow/.local/bin/airflow scheduler"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname \"$(hostname)\""]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 60s
    restart: unless-stopped
    depends_on:
      postgres-meta:
        condition: service_healthy
      airflow-211-init:
        condition: service_completed_successfully


  airflow-211-triggerer:
    <<: *airflow_common
    container_name: airflow-triggerer
    command: >
      bash -lc "exec airflow triggerer"
    # command: ["bash","-lc","exec /home/airflow/.local/bin/airflow triggerer"]
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type TriggererJob --hostname \"$(hostname)\""]
      interval: 10s
      timeout: 5s
      retries: 12
      start_period: 60s
    restart: unless-stopped
    depends_on:
      postgres-meta:
        condition: service_healthy
      airflow-211-init:
        condition: service_completed_successfully





  # # =========================
  # # Airflow (기본: LocalExecutor)
  # # =========================
  # airflow-init:
  #   <<: *airflow_common
  #   depends_on:
  #     postgres-meta:
  #       condition: service_healthy
  #   env_file: [.env]
  #   user: "50000:0"
  #   entrypoint: ["/bin/bash","-lc"]
  #   command: ["/opt/boot.sh"]
  #   restart: "no"                  # ← init 컨테이너는 재시작 금지




  # airflow-api-server:
  #   <<: *airflow_common
  #   depends_on:
  #     postgres-meta:
  #       condition: service_healthy
  #     airflow-scheduler:
  #       condition: service_started
  #   command: ["airflow", "api-server"]
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5
  #     start_period: 30s
  #   # ports: ["0.0.0.0:8080:8080"]   # 로컬 전용
  #   expose: ["8080"]


  # airflow-scheduler:
  #   <<: *airflow_common
  #   depends_on:
  #     postgres-meta:
  #       condition: service_healthy
  #     airflow-init:
  #       condition: service_completed_successfully
  #   # command: ["/bin/bash","-lc","airflow db check || airflow db migrate; exec airflow scheduler"]
  #   # command: ["scheduler", "db", "migrate"]
  #   command: ["scheduler"]





  # airflow-dag-processor:
  #   <<: *airflow_common
  #   depends_on:
  #     postgres-meta:
  #       condition: service_healthy
  #     airflow-init:
  #       condition: service_completed_successfully
  #   command: ["airflow","dag-processor"]
    





  # # airflow-worker:
  # #   <<: *airflow_common
  # #   command: celery worker
  # #   restart: always
  # #   depends_on:
  # #     airflow-api-server:
  # #       condition: service_healthy
  # #     airflow-init:
  # #       condition: service_completed_successfully
  # #   volumes: *airflow_vols



  # =========================
  # Cloudflare Tunnel
  # =========================
  cloudflared:
    image: cloudflare/cloudflared:latest
    command: tunnel --no-autoupdate run --token ${CF_TUNNEL_TOKEN}
    restart: unless-stopped
    networks: [edge]

networks:
  edge:
    driver: bridge

volumes:
  superset_home:
  meta_pg:
  data_pg:
